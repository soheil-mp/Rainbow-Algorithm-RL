{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<h1 style=\"text-align:center;\">Rainbow Algorithm Playing Atari Games</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Introduction\n",
    "\n",
    "---\n",
    "\n",
    "In this project, we aim to implement the Rainbow algorithm, a state-of-the-art reinforcement learning method that combines several key techniques to achieve superior performance in playing Atari games. The Rainbow algorithm integrates improvements from various sources, including Double Q-learning, Prioritized Experience Replay, Dueling Network Architectures, and more, to address the limitations of traditional Deep Q-Networks (DQN).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Noisy Linear\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, std_init=0.5):\n",
    "        super(NoisyLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.std_init = std_init\n",
    "        self.weight_mu = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        self.weight_sigma = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        self.register_buffer('weight_epsilon', torch.FloatTensor(out_features, in_features))\n",
    "        self.bias_mu = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        self.bias_sigma = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        self.register_buffer('bias_epsilon', torch.FloatTensor(out_features))\n",
    "        self.reset_parameters()\n",
    "        self.reset_noise()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        mu_range = 1 / np.sqrt(self.in_features)\n",
    "        self.weight_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.weight_sigma.data.fill_(self.std_init / np.sqrt(self.in_features))\n",
    "        self.bias_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.bias_sigma.data.fill_(self.std_init / np.sqrt(self.out_features))\n",
    "\n",
    "    def reset_noise(self):\n",
    "        epsilon_in = self._scale_noise(self.in_features)\n",
    "        epsilon_out = self._scale_noise(self.out_features)\n",
    "        self.weight_epsilon.copy_(epsilon_out.ger(epsilon_in))\n",
    "        self.bias_epsilon.copy_(self._scale_noise(self.out_features))\n",
    "\n",
    "    def _scale_noise(self, size):\n",
    "        x = torch.randn(size)\n",
    "        return x.sign().mul_(x.abs().sqrt_())\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.training:\n",
    "            return F.linear(input, self.weight_mu + self.weight_sigma * self.weight_epsilon,\n",
    "                            self.bias_mu + self.bias_sigma * self.bias_epsilon)\n",
    "        else:\n",
    "            return F.linear(input, self.weight_mu, self.bias_mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Prioritized Replay Buffer\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity, alpha=0.6, beta=0.4):\n",
    "        self.capacity = capacity\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.buffer = []\n",
    "        self.priorities = np.zeros((capacity,), dtype=np.float32)\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        max_priority = self.priorities.max() if self.buffer else 1.0\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append((state, action, reward, next_state, done))\n",
    "        else:\n",
    "            self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.priorities[self.position] = max_priority\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        if len(self.buffer) == self.capacity:\n",
    "            priorities = self.priorities\n",
    "        else:\n",
    "            priorities = self.priorities[:self.position]\n",
    "        \n",
    "        probs = priorities ** self.alpha\n",
    "        probs /= probs.sum()\n",
    "        \n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "        \n",
    "        weights = (len(self.buffer) * probs[indices]) ** (-self.beta)\n",
    "        weights /= weights.max()\n",
    "        \n",
    "        return samples, indices, weights\n",
    "\n",
    "    def update_priorities(self, indices, priorities):\n",
    "        for idx, priority in zip(indices, priorities):\n",
    "            self.priorities[idx] = priority + 1e-5  # Add small constant to avoid zero priority"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Rainbow Algorithm\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RainbowDQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, num_atoms, v_min, v_max):\n",
    "        super(RainbowDQN, self).__init__()\n",
    "        self.num_atoms = num_atoms\n",
    "        self.v_min = v_min\n",
    "        self.v_max = v_max\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        if isinstance(state_dim, int) or len(state_dim) == 1:\n",
    "            # For 1D state spaces (like CartPole)\n",
    "            self.features = nn.Sequential(\n",
    "                NoisyLinear(state_dim[0] if isinstance(state_dim, tuple) else state_dim, 128),\n",
    "                nn.ReLU(),\n",
    "                NoisyLinear(128, 128),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "            feature_output = 128\n",
    "        else:\n",
    "            # For image-based state spaces (like Atari games)\n",
    "            c, h, w = state_dim\n",
    "            self.features = nn.Sequential(\n",
    "                nn.Conv2d(c, 32, kernel_size=8, stride=4),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Flatten()\n",
    "            )\n",
    "            feature_output = self._get_conv_output(state_dim)\n",
    "        \n",
    "        self.advantage_stream = nn.Sequential(\n",
    "            NoisyLinear(feature_output, 512),\n",
    "            nn.ReLU(),\n",
    "            NoisyLinear(512, action_dim * num_atoms)\n",
    "        )\n",
    "        \n",
    "        self.value_stream = nn.Sequential(\n",
    "            NoisyLinear(feature_output, 512),\n",
    "            nn.ReLU(),\n",
    "            NoisyLinear(512, num_atoms)\n",
    "        )\n",
    "        \n",
    "    def _get_conv_output(self, shape):\n",
    "        o = self.features(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "        \n",
    "    def forward(self, state):\n",
    "        features = self.features(state)\n",
    "        advantage = self.advantage_stream(features).view(-1, self.action_dim, self.num_atoms)\n",
    "        value = self.value_stream(features).view(-1, 1, self.num_atoms)\n",
    "        q_dist = value + advantage - advantage.mean(dim=1, keepdim=True)\n",
    "        return F.softmax(q_dist, dim=-1)\n",
    "\n",
    "    def reset_noise(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, NoisyLinear):\n",
    "                module.reset_noise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RainbowAgent:\n",
    "    def __init__(self, state_dim, action_dim, lr=0.0001, gamma=0.99, num_atoms=51, v_min=-10, v_max=10, buffer_size=100000, batch_size=32):\n",
    " \n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.num_atoms = num_atoms\n",
    "        self.v_min = v_min\n",
    "        self.v_max = v_max\n",
    "        self.batch_size = batch_size\n",
    "        self.state_dim = state_dim\n",
    "        \n",
    "        self.policy_net = RainbowDQN(state_dim, action_dim, num_atoms, v_min, v_max).to(self.device)\n",
    "        self.target_net = RainbowDQN(state_dim, action_dim, num_atoms, v_min, v_max).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        self.memory = PrioritizedReplayBuffer(buffer_size)\n",
    "        \n",
    "        self.support = torch.linspace(v_min, v_max, num_atoms).to(self.device)\n",
    "        self.delta_z = (v_max - v_min) / (num_atoms - 1)\n",
    "        \n",
    "        self.state_dim = state_dim\n",
    "        \n",
    "    def preprocess_state(self, state):\n",
    "        if len(self.state_dim) == 3:\n",
    "            # For image-based states (like Atari)\n",
    "            if len(state.shape) == 3 and state.shape[2] == 3:  # If the state is in RGB\n",
    "                state = cv2.cvtColor(state, cv2.COLOR_RGB2GRAY)\n",
    "            elif len(state.shape) == 3:  # If the state is already grayscale but has 3 dimensions\n",
    "                state = state[:, :, 0]\n",
    "            # Resize the image\n",
    "            state = cv2.resize(state, (84, 84), interpolation=cv2.INTER_AREA)\n",
    "            state = np.expand_dims(state, axis=0)  # Add channel dimension\n",
    "        return state.astype(np.float32) / 255.0  # Normalize the state\n",
    "        \n",
    "    def select_action(self, state, epsilon=0.01):\n",
    "        if random.random() < epsilon:\n",
    "            return random.randrange(self.action_dim)\n",
    "        with torch.no_grad():\n",
    "            state = self.preprocess_state(state)\n",
    "            state = torch.FloatTensor(state).to(self.device)\n",
    "            if state.dim() == 3:\n",
    "                state = state.unsqueeze(0)  # Add batch dimension if not present\n",
    "            dist = self.policy_net(state).data.cpu()\n",
    "            dist = dist * self.support.cpu()\n",
    "            action = dist.sum(2).max(1)[1].item()\n",
    "        return action\n",
    "    \n",
    "    # ... (rest of the RainbowAgent class remains the same)\n",
    "    \n",
    "    def update(self):\n",
    "        if len(self.memory.buffer) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        transitions, indices, weights = self.memory.sample(self.batch_size)\n",
    "        batch = list(zip(*transitions))\n",
    "        \n",
    "        state_batch = torch.FloatTensor(np.array(batch[0])).to(self.device)\n",
    "        action_batch = torch.LongTensor(np.array(batch[1])).to(self.device)\n",
    "        reward_batch = torch.FloatTensor(np.array(batch[2])).to(self.device)\n",
    "        next_state_batch = torch.FloatTensor(np.array(batch[3])).to(self.device)\n",
    "        done_batch = torch.FloatTensor(np.array(batch[4])).to(self.device)\n",
    "        \n",
    "        # Compute current Q-values\n",
    "        current_q_dist = self.policy_net(state_batch)\n",
    "        current_q_dist = current_q_dist[range(self.batch_size), action_batch]\n",
    "        \n",
    "        # Compute next Q-values\n",
    "        with torch.no_grad():\n",
    "            next_q_dist = self.target_net(next_state_batch)\n",
    "            best_actions = (next_q_dist * self.support).sum(2).max(1)[1]\n",
    "            next_q_dist = next_q_dist[range(self.batch_size), best_actions]\n",
    "            \n",
    "            # Compute projected distribution\n",
    "            projected_dist = self._categorical_projection(next_q_dist, reward_batch, done_batch)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = -(projected_dist * current_q_dist.log()).sum(1)\n",
    "        priorities = loss.detach().cpu().numpy()  # This is now an array\n",
    "        loss = (loss * torch.FloatTensor(weights).to(self.device)).mean()\n",
    "        \n",
    "        # Update priorities\n",
    "        self.memory.update_priorities(indices, priorities)\n",
    "        \n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 10)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Reset noisy layers\n",
    "        self.policy_net.reset_noise()\n",
    "        self.target_net.reset_noise()\n",
    "\n",
    "    def _categorical_projection(self, next_q_dist, rewards, dones):\n",
    "        batch_size = len(rewards)\n",
    "        projected_dist = torch.zeros(batch_size, self.num_atoms).to(self.device)\n",
    "        \n",
    "        rewards = rewards.unsqueeze(1).expand_as(projected_dist)\n",
    "        dones = dones.unsqueeze(1).expand_as(projected_dist)\n",
    "        support = self.support.unsqueeze(0).expand_as(projected_dist)\n",
    "        \n",
    "        tz = rewards + (1 - dones) * self.gamma * support\n",
    "        tz = tz.clamp(min=self.v_min, max=self.v_max)\n",
    "        b = (tz - self.v_min) / self.delta_z\n",
    "        l = b.floor().long()\n",
    "        u = b.ceil().long()\n",
    "        \n",
    "        l[(u > 0) * (l == u)] -= 1\n",
    "        u[(l < (self.num_atoms - 1)) * (l == u)] += 1\n",
    "        \n",
    "        offset = torch.linspace(0, (batch_size - 1) * self.num_atoms, batch_size).long().unsqueeze(1).expand(batch_size, self.num_atoms).to(self.device)\n",
    "        \n",
    "        projected_dist.view(-1).index_add_(0, (l + offset).view(-1), (next_q_dist * (u.float() - b)).view(-1))\n",
    "        projected_dist.view(-1).index_add_(0, (u + offset).view(-1), (next_q_dist * (b - l.float())).view(-1))\n",
    "        \n",
    "        return projected_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Training\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def train(env_name, num_episodes=1000, save_interval=100, save_dir='saved_models'):\n",
    "    env = gym.make(env_name)\n",
    "    \n",
    "    # Determine the state dimension based on the environment\n",
    "    if len(env.observation_space.shape) == 3:\n",
    "        state_dim = (1, 84, 84)  # We'll preprocess to this size for Atari games\n",
    "    else:\n",
    "        state_dim = env.observation_space.shape\n",
    "    \n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "    agent = RainbowAgent(state_dim, action_dim)\n",
    "\n",
    "    # Create directory for saving models if it doesn't exist\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        state = agent.preprocess_state(state)\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            next_state = agent.preprocess_state(next_state)\n",
    "            done = terminated or truncated\n",
    "            total_reward += reward\n",
    "            \n",
    "            agent.memory.push(state, action, reward, next_state, done)\n",
    "            agent.update()\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        if episode % 10 == 0:\n",
    "            agent.target_net.load_state_dict(agent.policy_net.state_dict())\n",
    "        \n",
    "        print(f\"Episode {episode}, Total Reward: {total_reward}\")\n",
    "\n",
    "        # Save model periodically\n",
    "        if (episode + 1) % save_interval == 0:\n",
    "            save_path = os.path.join(save_dir, f\"{env_name}_rainbow_episode_{episode+1}.pth\")\n",
    "            torch.save(agent.policy_net.state_dict(), save_path)\n",
    "            print(f\"Model saved to {save_path}\")\n",
    "\n",
    "    # Save final model\n",
    "    final_save_path = os.path.join(save_dir, f\"{env_name}_rainbow_final.pth\")\n",
    "    torch.save(agent.policy_net.state_dict(), final_save_path)\n",
    "    print(f\"Final model saved to {final_save_path}\")\n",
    "\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on CartPole-v1\n",
      "Episode 0, Total Reward: 9.0\n",
      "Episode 1, Total Reward: 9.0\n",
      "Episode 2, Total Reward: 9.0\n",
      "Episode 3, Total Reward: 11.0\n",
      "Episode 4, Total Reward: 19.0\n",
      "Episode 5, Total Reward: 9.0\n",
      "Episode 6, Total Reward: 10.0\n",
      "Episode 7, Total Reward: 10.0\n",
      "Episode 8, Total Reward: 11.0\n",
      "Episode 9, Total Reward: 10.0\n",
      "Episode 10, Total Reward: 10.0\n",
      "Episode 11, Total Reward: 9.0\n",
      "Episode 12, Total Reward: 11.0\n",
      "Episode 13, Total Reward: 9.0\n",
      "Episode 14, Total Reward: 10.0\n",
      "Episode 15, Total Reward: 10.0\n",
      "Episode 16, Total Reward: 9.0\n",
      "Episode 17, Total Reward: 10.0\n",
      "Episode 18, Total Reward: 10.0\n",
      "Episode 19, Total Reward: 10.0\n",
      "Episode 20, Total Reward: 9.0\n",
      "Episode 21, Total Reward: 9.0\n",
      "Episode 22, Total Reward: 10.0\n",
      "Episode 23, Total Reward: 10.0\n",
      "Episode 24, Total Reward: 9.0\n",
      "Episode 25, Total Reward: 9.0\n",
      "Episode 26, Total Reward: 9.0\n",
      "Episode 27, Total Reward: 10.0\n",
      "Episode 28, Total Reward: 9.0\n",
      "Episode 29, Total Reward: 10.0\n",
      "Episode 30, Total Reward: 10.0\n",
      "Episode 31, Total Reward: 10.0\n",
      "Episode 32, Total Reward: 10.0\n",
      "Episode 33, Total Reward: 10.0\n",
      "Episode 34, Total Reward: 9.0\n",
      "Episode 35, Total Reward: 9.0\n",
      "Episode 36, Total Reward: 10.0\n",
      "Episode 37, Total Reward: 10.0\n",
      "Episode 38, Total Reward: 9.0\n",
      "Episode 39, Total Reward: 10.0\n",
      "Episode 40, Total Reward: 10.0\n",
      "Episode 41, Total Reward: 8.0\n",
      "Episode 42, Total Reward: 10.0\n",
      "Episode 43, Total Reward: 9.0\n",
      "Episode 44, Total Reward: 8.0\n",
      "Episode 45, Total Reward: 10.0\n",
      "Episode 46, Total Reward: 10.0\n",
      "Episode 47, Total Reward: 10.0\n",
      "Episode 48, Total Reward: 8.0\n",
      "Episode 49, Total Reward: 10.0\n",
      "Episode 50, Total Reward: 10.0\n",
      "Episode 51, Total Reward: 9.0\n",
      "Episode 52, Total Reward: 9.0\n",
      "Episode 53, Total Reward: 11.0\n",
      "Episode 54, Total Reward: 11.0\n",
      "Episode 55, Total Reward: 9.0\n",
      "Episode 56, Total Reward: 9.0\n",
      "Episode 57, Total Reward: 9.0\n",
      "Episode 58, Total Reward: 9.0\n",
      "Episode 59, Total Reward: 9.0\n",
      "Episode 60, Total Reward: 9.0\n",
      "Episode 61, Total Reward: 9.0\n",
      "Episode 62, Total Reward: 9.0\n",
      "Episode 63, Total Reward: 8.0\n",
      "Episode 64, Total Reward: 9.0\n",
      "Episode 65, Total Reward: 10.0\n",
      "Episode 66, Total Reward: 8.0\n",
      "Episode 67, Total Reward: 10.0\n",
      "Episode 68, Total Reward: 10.0\n",
      "Episode 69, Total Reward: 10.0\n",
      "Episode 70, Total Reward: 9.0\n",
      "Episode 71, Total Reward: 10.0\n",
      "Episode 72, Total Reward: 9.0\n",
      "Episode 73, Total Reward: 10.0\n",
      "Episode 74, Total Reward: 9.0\n",
      "Episode 75, Total Reward: 10.0\n",
      "Episode 76, Total Reward: 9.0\n",
      "Episode 77, Total Reward: 8.0\n",
      "Episode 78, Total Reward: 9.0\n",
      "Episode 79, Total Reward: 9.0\n",
      "Episode 80, Total Reward: 11.0\n",
      "Episode 81, Total Reward: 9.0\n",
      "Episode 82, Total Reward: 8.0\n",
      "Episode 83, Total Reward: 9.0\n",
      "Episode 84, Total Reward: 9.0\n",
      "Episode 85, Total Reward: 10.0\n",
      "Episode 86, Total Reward: 10.0\n",
      "Episode 87, Total Reward: 10.0\n",
      "Episode 88, Total Reward: 10.0\n",
      "Episode 89, Total Reward: 10.0\n",
      "Episode 90, Total Reward: 10.0\n",
      "Episode 91, Total Reward: 9.0\n",
      "Episode 92, Total Reward: 11.0\n",
      "Episode 93, Total Reward: 10.0\n",
      "Episode 94, Total Reward: 9.0\n",
      "Episode 95, Total Reward: 10.0\n",
      "Episode 96, Total Reward: 9.0\n",
      "Episode 97, Total Reward: 9.0\n",
      "Episode 98, Total Reward: 9.0\n",
      "Episode 99, Total Reward: 9.0\n",
      "Model saved to saved_models\\CartPole-v1_rainbow_episode_100.pth\n",
      "Final model saved to saved_models\\CartPole-v1_rainbow_final.pth\n"
     ]
    }
   ],
   "source": [
    "# Test on CartPole\n",
    "print(\"Training on CartPole-v1\")\n",
    "train(\"CartPole-v1\", num_episodes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training on SpaceInvaders-v4\n",
      "Episode 0, Total Reward: 435.0\n",
      "Episode 1, Total Reward: 280.0\n",
      "Episode 2, Total Reward: 295.0\n",
      "Episode 3, Total Reward: 240.0\n",
      "Episode 4, Total Reward: 355.0\n",
      "Episode 5, Total Reward: 100.0\n",
      "Episode 6, Total Reward: 385.0\n",
      "Episode 7, Total Reward: 210.0\n",
      "Episode 8, Total Reward: 120.0\n",
      "Episode 9, Total Reward: 80.0\n",
      "Final model saved to saved_models\\ALE/SpaceInvaders-v5_rainbow_final.pth\n"
     ]
    }
   ],
   "source": [
    "# Test on Space Invaders\n",
    "print(\"\\nTraining on SpaceInvaders-v4\")\n",
    "train(\"ALE/SpaceInvaders-v5\", num_episodes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Inference\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(env_name, model_path, num_episodes=10, render=True):\n",
    "    env = gym.make(env_name, render_mode=\"human\" if render else None)\n",
    "    \n",
    "    # Determine the state dimension based on the environment\n",
    "    if len(env.observation_space.shape) == 3:\n",
    "        state_dim = (1, 84, 84)  # Preprocessed image size for Atari games\n",
    "    else:\n",
    "        state_dim = env.observation_space.shape\n",
    "    \n",
    "    action_dim = env.action_space.n\n",
    "    \n",
    "    # Create and load the agent\n",
    "    agent = RainbowAgent(state_dim, action_dim)\n",
    "    agent.policy_net.load_state_dict(torch.load(model_path))\n",
    "    agent.policy_net.eval()  # Set the network to evaluation mode\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            if render:\n",
    "                env.render()\n",
    "            \n",
    "            action = agent.select_action(state, epsilon=0)  # Use epsilon=0 for greedy action selection\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total_reward += reward\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        print(f\"Episode {episode}, Total Reward: {total_reward}\")\n",
    "    \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = 'saved_models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferencing CartPole-v1:\n",
      "Episode 0, Total Reward: 13.0\n",
      "Episode 1, Total Reward: 13.0\n",
      "Episode 2, Total Reward: 22.0\n",
      "Episode 3, Total Reward: 27.0\n",
      "Episode 4, Total Reward: 11.0\n",
      "Episode 5, Total Reward: 14.0\n",
      "Episode 6, Total Reward: 21.0\n",
      "Episode 7, Total Reward: 18.0\n",
      "Episode 8, Total Reward: 24.0\n",
      "Episode 9, Total Reward: 22.0\n"
     ]
    }
   ],
   "source": [
    "# Inference for CartPole\n",
    "env_name = \"CartPole-v1\"\n",
    "model_path = os.path.join(save_dir, f\"{env_name}_rainbow_final.pth\")\n",
    "print(\"Inferencing CartPole-v1:\")\n",
    "inference(env_name, model_path, num_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inferencing SpaceInvaders-v5:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\soheil\\.conda\\envs\\rl_agent\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:335: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Total Reward: 175.0\n",
      "Episode 1, Total Reward: 130.0\n",
      "Episode 2, Total Reward: 165.0\n",
      "Episode 3, Total Reward: 340.0\n",
      "Episode 4, Total Reward: 160.0\n"
     ]
    }
   ],
   "source": [
    "# Inference for Space Invaders\n",
    "env_name = \"ALE/SpaceInvaders-v5\"\n",
    "model_path = os.path.join(save_dir, f\"{env_name}_rainbow_final.pth\")\n",
    "# model_path = \"saved_models/ALE/SpaceInvaders-v5_rainbow_episode_100.pth\"\n",
    "print(\"\\nInferencing SpaceInvaders-v5:\")\n",
    "inference(env_name, model_path, num_episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
